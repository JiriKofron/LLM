# Show Token Mappings Documentation

## Overview
This script demonstrates how tokens are mapped to IDs in different models (BERT and GPT-2), showing the relationship between text, tokens, and their corresponding IDs.

## Key Concepts

### Token Mapping
- How text is converted to tokens
- How tokens get unique IDs
- How IDs are used in models
- How mappings are consistent

### Model Differences
- BERT vs GPT-2 tokenization
- Different vocabulary sizes
- Different tokenization strategies
- Different handling of special tokens

### Mapping Analysis
- How to analyze token mappings
- How to understand token IDs
- How to handle special cases
- How to compare models

## Usage
```python
python scripts/show_token_mappings.py
```

## Output
The script will show:
1. Token to ID mappings
2. Model differences
3. Special token handling
4. Vocabulary comparisons
5. Key points about mappings

## Related Files
- `show_token_mappings.py`: The main script
- `requirements.txt`: Required dependencies

## Further Reading
- [BERT Paper](https://arxiv.org/abs/1810.04805)
- [GPT-2 Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) 